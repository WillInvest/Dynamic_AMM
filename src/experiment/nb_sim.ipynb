{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing src directory\n",
    "import sys\n",
    "sys.path.append('/Users/andrewcarranti/CODE/SHIFT/2024/py_repo/post_refactor/AMM-Python/src')\n",
    "# experiment imports\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime as dt\n",
    "from scipy.stats import truncnorm\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from scipy.optimize import minimize\n",
    "# project imports\n",
    "from amm.amm import AMM, SimpleFeeAMM\n",
    "from amm.fee import TriangleFee, PercentFee, NoFee\n",
    "# data imports\n",
    "from data.kaiko import fetch_data\n",
    "from api_key.my_api_key import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gbm_assumption_test(log_returns):\n",
    "    adf_result = adfuller(log_returns) # check for stationarity\n",
    "    print(\"ADF Statistic:\", adf_result[0]) # check for stationarity\n",
    "    print(\"P-value:\", adf_result[1])\n",
    "    print(\"Critical Values:\", adf_result[4])\n",
    "    print(\"Stationary:\", adf_result[1] <= 0.05)\n",
    "    # if adf_result[1] > 0.05:  # if not stationary, iteratively difference until achieved\n",
    "    #     for d in range(1, max_lag + 1):\n",
    "    #         diff_data = diff(log_returns, k_diff=d)\n",
    "    #         adf_result = adfuller(diff_data)\n",
    "    #         print(f\"ADF result after differencing level {d}: {adf_result[0]}, p-value: {adf_result[1]}\")\n",
    "    #         if adf_result[1] <= 0.05:\n",
    "    #             print(\"Achieved stationarity with differencing level:\", d)\n",
    "    #             diff_data = diff_data\n",
    "    #             break\n",
    "    shapiro_result = shapiro(log_returns) # check for normality\n",
    "    print(\"Shapiro-Wilk Test Statistic:\", shapiro_result[0])\n",
    "    print(\"p-value:\", shapiro_result[1])\n",
    "    print(\"normal:\", shapiro_result[1] > 0.05)\n",
    "    lb_result = acorr_ljungbox(log_returns, lags=[10], return_df=True) # check for independence (autocorrelation)\n",
    "    print(\"Ljung-Box test:\")\n",
    "    print(lb_result)\n",
    "    print(\"Independent:\", lb_result['lb_pvalue'].iloc[0] > 0.05)\n",
    "    print(\"-\"*50)\n",
    "    # if lb_result['lb_pvalue'].iloc[0] < 0.05: # if autocorrelation detected, adjust\n",
    "    #     print(\"Autocorrelation detected:\") \n",
    "    #     plot_pacf(log_returns, lags=40) # plot partial autocorrelation function\n",
    "    #     plt.title('Partial Autocorrelation Function (PACF)')\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the log-likelihood function\n",
    "def neg_log_likelihood(params, log_returns):\n",
    "    \"\"\"\n",
    "    calculate negative log likelihood of a normal distribution for calibrating GBM\n",
    "    params: tuple, mu and sigma\n",
    "    \"\"\"\n",
    "    mu, sigma = params # define mu and sigma\n",
    "    estimated_mu = np.mean(log_returns) # estimate mu\n",
    "    estimated_var = np.sum((log_returns - estimated_mu)**2) / len(log_returns) # estimate variance\n",
    "    return 0.5 * len(log_returns) * np.log(2 * np.pi * estimated_var) + 0.5 / estimated_var * np.sum((log_returns - mu)**2) # return negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_gbm(asset, data, frequency, T, N, type):\n",
    "    \"\"\"\n",
    "    calibrate geometric brownian motion for next period (t=0 is last observation in data)\n",
    "    \n",
    "    calibrate gbm model by pulling data from kaiko api\n",
    "\n",
    "    asset (str): asset to calibrate\n",
    "    data (pd.DataFrame): price data w/ column 'price'\n",
    "    freq (str): frequency of data (1h, 1d, 1w)\n",
    "    T (float): terminal time\n",
    "    N (int): number of time steps\n",
    "    type (str): type of calibration (reg, mle)\n",
    "    max_lag (int): maximum lag for autocorrelation test (default=10)\n",
    "    alpha (float): significance level for hypothesis tests (default=0.05)\n",
    "\n",
    "    return mu (float), sigma (float), S (numpy.ndarray)\n",
    "    \"\"\"\n",
    "    \n",
    "    if type == \"reg\":\n",
    "        print(\"dfghjk\")\n",
    "\n",
    "        returns = np.log((data / data.shift(1)).dropna()) # get returns\n",
    "        gbm_assumption_test(returns) # test gbm assumptions\n",
    "        mu = returns.mean() * 365.25  # annualized return\n",
    "\n",
    "        print(\"dfghjk\")\n",
    "\n",
    "        sigma = returns.std() * 365.25 ** 0.5 # annualized volatility\n",
    "        print(f'Estimated {asset} {frequency} Mu:', round(mu, 2), 'Estimated Annualized Mu:', round(mu * 365.25,2))\n",
    "        print(f'Estimated {asset} {frequency} Sigma:', round(sigma, 2), 'Estimated Annualized Sigma:', round(sigma * 365.25**0.5, 2))\n",
    "        S0 = data.iloc[-1] # get LAST price in series\n",
    "        dt = T / N # time step size\n",
    "        t = np.linspace(0, T, N)\n",
    "        W = np.random.standard_normal(size=N)\n",
    "        W = np.cumsum(W) * np.sqrt(dt)  # Standard Brownian motion\n",
    "        X = (mu - 0.5 * sigma**2) * t + sigma * W \n",
    "        S = S0 * np.exp(X)  # Geometric Brownian motion   \n",
    "\n",
    "        print(\"dfghjk\")\n",
    "\n",
    "        return mu, sigma, S\n",
    "    \n",
    "    elif type == \"mle\":\n",
    "        log_returns = np.log(1 + data.pct_change().dropna()) # calculate log returns\n",
    "        result = minimize(neg_log_likelihood, [0.05, 0.2], args=(log_returns,), bounds=((None, None), (1e-4, None))) # minimize the negative log-likelihood\n",
    "        mu = result.x[0] * 365.25 # annualize mu\n",
    "        sigma = result.x[1] * 365.25**0.5 # annualize sigma\n",
    "        print(f'Estimated {asset} {frequency} Mu:', round(result.x[0],5), 'Estimated Annualized Mu:', round(mu, 5)) # using 365.25 instead of 252 bcs operate 24/7\n",
    "        print(f'Estimated {asset} {frequency} Sigma:', round(result.x[1],5), 'Estimated Annualized Sigma:', round(sigma, 5))\n",
    "        S0 = data.iloc[-1] # get LAST price in series\n",
    "        dt = T / N # time step size\n",
    "        t = np.linspace(0, T, N)\n",
    "        W = np.random.standard_normal(size=N)\n",
    "        W = np.cumsum(W) * np.sqrt(dt)  # standard BM\n",
    "        X = (mu - 0.5 * sigma**2) * t + sigma * W \n",
    "        S = S0 * np.exp(X)  # GBM\n",
    "        return mu, sigma, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gbm_data(pair, start_date, end_date, freq, api_key):\n",
    "    \"\"\"\n",
    "    get gbm data from kaiko api or local storage\n",
    "\n",
    "    asset (str): asset symbol\n",
    "    start_date (str): start date of data\n",
    "    end_date (str): end date of data\n",
    "    freq (str): frequency of data (1h, 1d, 1w)\n",
    "    api_key (str): kaiko api key\n",
    "\n",
    "    return pd.DataFrame: price data\n",
    "    \"\"\"\n",
    "    # check if data exists, if not fetch data\n",
    "    asset1 = pair.split(\"-\")[0] \n",
    "    asset2 = pair.split(\"-\")[1]\n",
    "\n",
    "    if os.path.exists(f\"/data/crypto_data/{asset1}-usd_{start_date}_{end_date}_{freq}.csv\"):\n",
    "        data1 =  pd.read_csv(f\"/data/crypto_data/{asset1}-usd_{start_date}_{end_date}_{freq}.csv\")[\"price\"]\n",
    "    else: data1 = fetch_data(api_key, asset1+\"-usd\", start_date, end_date, freq)\n",
    "    data1['timestamp'] = pd.to_datetime(data1['timestamp'], unit='ms') # convert timestamp to datetime\n",
    "    data1['price'] = pd.to_numeric(data1['price'])\n",
    "\n",
    "    if os.path.exists(f\"/data/crypto_data/{asset2}-usd_{start_date}_{end_date}_{freq}.csv\"):\n",
    "        data2 =  pd.read_csv(f\"/data/crypto_data/{asset2}-usd_{start_date}_{end_date}_{freq}.csv\")[\"price\"]\n",
    "    else: data2 = fetch_data(api_key, asset2+\"-usd\", start_date, end_date, freq)\n",
    "    data2['timestamp'] = pd.to_datetime(data2['timestamp'], unit='ms') # convert timestamp to datetime\n",
    "    data2['price'] = pd.to_numeric(data2['price'])\n",
    "\n",
    "    return pd.merge(data1, data2, on='timestamp', how='inner', suffixes=(\"_\" + asset1, \"_\" + asset2)) # merge dataframes on timestamp saving price for each asset denominated in USD for storing AMM market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sim1(n, pair, start_dt, end_dt, frequency, L0=1000000, spread=0.5):\n",
    "    \"\"\"\n",
    "    simulate AMM market with data calibrated GBM for external oracles and trading agents\n",
    "    n (int): number of simulations\n",
    "    pair (str): asset pair for data (e.g. btc-eth)\n",
    "    asset1_n (int): number of asset1 tokens\n",
    "    asset2_n (int): number of asset2 tokens\n",
    "    start_dt (str): start date for data (YYYY-MM-DD)\n",
    "    end_dt (str): end date for data (YYYY-MM-DD)\n",
    "    frequency (str): frequency of data (1h, 1d, 1w)\n",
    "    L0 (int): number of initial LP tokens\n",
    "    spread (float): spread for arbitrage agents (e.g. 0.5%)\n",
    "    return list: list of dataframes for each simulation \n",
    "    \"\"\"\n",
    "\n",
    "    # # SIM STORAGE # #\n",
    "    # create list to store dfs from each simulation of amms\n",
    "    sim_amm_dfs= []\n",
    "    sim_amms = []\n",
    "    # parse asset1 and asset2, create USD denominated pairs\n",
    "    asset1 = pair.split(\"-\")[0] \n",
    "    asset2 = pair.split(\"-\")[1]\n",
    "\n",
    "    # # DATA & GBM CALIBRATION # #\n",
    "    difference = dt.strptime(end_dt, '%Y-%m-%dT%H:%M:%SZ') - dt.strptime(start_dt, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    T_years = difference.days / 365.25  # using 365.25 to account for leap years\n",
    "    marketDF =  get_gbm_data(pair, start_dt, end_dt, frequency, api_key) # get data for assets\n",
    "    n_timesteps = len(marketDF) # number of timesteps in data\n",
    "    new_cols = [f'gbm_price_{asset1}', f'gbm_price_{asset2}', # inventory of each asset\n",
    "                f'{asset1}_inv', f'{asset2}_inv', 'L_inv', f'F{asset1}_inv', f'F{asset2}_inv', \n",
    "                'FL_inv', f'dt_{asset1}', f'dt_{asset2}', 'dt_L', f'dt_F{asset1}', f'dt_L']\n",
    "    marketDF = marketDF.assign(**{col: None for col in new_cols})\n",
    "    \n",
    "    L0 = 1000000 # 1 mil LP tokens\n",
    "    A0 = math.sqrt(L0**2 * marketDF[f'price_{asset1}'][0]/marketDF[f'price_{asset2}'][0]) # evenly distribute assets\n",
    "    B0 = L0**2 / A0\n",
    "    market_portfolio = {\"A\": A0, \"B\": B0, \"L\":L0} # initial portfolio \n",
    "    \n",
    "    # gbm_assumption_test(np.log(1 + marketDF[f\"price_{asset1}\"].pct_change().dropna())) # test gbm assumptions\n",
    "    # gbm_assumption_test(np.log(1 + marketDF[f\"price_{asset2}\"].pct_change().dropna())) # test gbm assumptions\n",
    "\n",
    "    # # TIME SERIES SIMULATIONS # #\n",
    "    \n",
    "\n",
    "    # # TODO: ISSUE BELOW\n",
    "    for simulation in range(n): # for each simulation create new set of amms & run new set of trades\n",
    "        _,_,marketDF[f'gbm_price_{asset1}'] = calibrate_gbm(asset1, marketDF[f\"price_{asset1}\"], frequency, T_years, n_timesteps, \"mle\") # calibrate gbm for asset1 w/ MLE\n",
    "        _,_,marketDF[f'gbm_price_{asset2}'] = calibrate_gbm(asset2, marketDF[f\"price_{asset2}\"], frequency, T_years, n_timesteps, \"mle\") # calibrate gbm for asset2 w/ MLE\n",
    "        marketDF[f'amm_{asset1}/{asset2}'][0] = A0/B0 # set initial amm ratio\n",
    "\n",
    "        nofeeAMM = SimpleFeeAMM(fee_structure = NoFee(), initial_portfolio=market_portfolio)\n",
    "        percentAMM = SimpleFeeAMM(fee_structure = PercentFee(0.01), initial_portfolio=market_portfolio)\n",
    "        triAMM = SimpleFeeAMM(fee_structure = TriangleFee(0.003, 0.0001, -1), initial_portfolio=market_portfolio) \n",
    "        percentDF = marketDF.copy(deep=True)\n",
    "        nofeeDF = marketDF.copy(deep=True)\n",
    "        triDF = marketDF.copy(deep=True)\n",
    "        amms = [(nofeeAMM, nofeeDF), (percentAMM, percentDF), (triAMM, triDF)] # store pairs of amm type & df for updating\n",
    "        \n",
    "# TODO: fix here ^\n",
    "        # # SIMULATION # #\n",
    "        for t in range(n_timesteps): # iterate over each timestep in crypto market data\n",
    "\n",
    "            # print(marketDF[f'amm_{asset1}/{asset2}'][t])\n",
    "            # print((marketDF[f'gbm_{asset1}/{asset2}'][t] * (1+spread/100)))\n",
    "            # # ARBITRAGE AGENT # #\n",
    "            \n",
    "            for amm, df in amms: # update market data with amm data\n",
    "\n",
    "                if marketDF[f'amm_{asset1}/{asset2}'][t] > (marketDF[f'gbm_{asset1}/{asset2}'][t] * (1+spread/100)): # rule-based arbitrage agents in the market\n",
    "                    asset_out, asset_in, asset_in_n = asset1, asset2, random.choice(list(range(1, 50))) # modeling market efficiency\n",
    "                if (marketDF[f'amm_{asset1}/{asset2}'][t] * 1.005) < marketDF[f'gbm_{asset1}/{asset2}'][t]:\n",
    "                    asset_out, asset_in, asset_in_n = asset2, asset1, random.choice(list(range(1, 50)))\n",
    "                else: continue\n",
    "\n",
    "\n",
    "\n",
    "                succ, info = amm.trade_swap(asset_out, asset_in, asset_in_n) # call trade for each AMM\n",
    "                new_row = {f'{asset1}_inv': amm.portfolio[asset1], f'{asset2}_inv': amm.portfolio[asset2], # add trade info to df\n",
    "                           'LInv': amm.portfolio['L'], asset1: info['asset_delta'][asset1], \n",
    "                           f'{asset2}': info['asset_delta'][asset2], 'L': info['asset_delta']['L'], \n",
    "                        f'F{asset1}': amm.fees[asset1], f'F{asset2}': amm.fees[asset2], 'FL': amm.fees['L']}\n",
    "                df.loc[t] = new_row # append new row to df\n",
    "\n",
    "        for amm, df in amms:\n",
    "            sim_amm_dfs.append(df)\n",
    "            sim_amms.append(amm)\n",
    "    return sim_amm_dfs, sim_amms # return list of dfs for each simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTES FROM LAST MEETING:\n",
    "# FOCUS MORE ON TESTING FEES THROUGH SIM\n",
    "\n",
    "# # EXPERIMENTS TODO: # #\n",
    "# [1] run for large simulations and evaluate over time - explore different time periods to test from (different market conditions and lengths of historical windows) and different frequencies (1h, 1d, 1w)\n",
    "# [2] identify GBM paths that deplete pools (depletion of liquidity) and have both fall in value (impermanent loss) to show how fee accumulation compares ot general trend (law of large #s)\n",
    "        # impermanent loss evaluation could allow for an expected value calculation for LP returns (expected value of fees vs. impermanent loss)\n",
    "# [3] use stock data to see how compares\n",
    "# [4] make sure to highlight how different fee AMMs (basically fees) are affected by different market conditions and therefore how fee accumulation is affected\n",
    "\n",
    "# # UPDATES # #\n",
    "# [1] *importing stock data to use instead of crypto (more in line with goal application and can properly use GBM to simulate)\n",
    "# [2] considering train/test split for calibrating GBM and simulating trades source data (not overly urgent given not forecasting)\n",
    "# [3] maybe also considering changing source data from vwap if stick with crypto data\n",
    "        # multiple price streams for multiple external oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated btc 1d Mu: 0.00251 Estimated Annualized Mu: 0.91566\n",
      "Estimated btc 1d Sigma: 0.2 Estimated Annualized Sigma: 3.8223\n",
      "Estimated eth 1d Mu: 0.00195 Estimated Annualized Mu: 0.71192\n",
      "Estimated eth 1d Sigma: 0.2 Estimated Annualized Sigma: 3.8223\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'amm_btc/eth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'amm_btc/eth'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msim1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbtc-eth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-02-01T00:00:00Z\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2024-03-01T00:00:00Z\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 49\u001b[0m, in \u001b[0;36msim1\u001b[0;34m(n, pair, start_dt, end_dt, frequency, L0, spread)\u001b[0m\n\u001b[1;32m     47\u001b[0m _,_,marketDF[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbm_price_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m calibrate_gbm(asset1, marketDF[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], frequency, T_years, n_timesteps, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# calibrate gbm for asset1 w/ MLE\u001b[39;00m\n\u001b[1;32m     48\u001b[0m _,_,marketDF[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbm_price_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m calibrate_gbm(asset2, marketDF[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], frequency, T_years, n_timesteps, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# calibrate gbm for asset2 w/ MLE\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mmarketDF\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamm_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43masset1\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43masset2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m A0\u001b[38;5;241m/\u001b[39mB0 \u001b[38;5;66;03m# set initial amm ratio\u001b[39;00m\n\u001b[1;32m     51\u001b[0m nofeeAMM \u001b[38;5;241m=\u001b[39m SimpleFeeAMM(fee_structure \u001b[38;5;241m=\u001b[39m NoFee(), initial_portfolio\u001b[38;5;241m=\u001b[39mmarket_set_portfolio)\n\u001b[1;32m     52\u001b[0m percentAMM \u001b[38;5;241m=\u001b[39m SimpleFeeAMM(fee_structure \u001b[38;5;241m=\u001b[39m PercentFee(\u001b[38;5;241m0.01\u001b[39m), initial_portfolio\u001b[38;5;241m=\u001b[39mmarket_set_portfolio)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'amm_btc/eth'"
     ]
    }
   ],
   "source": [
    "\n",
    "sim1(2, \"btc-eth\", '2023-02-01T00:00:00Z', '2024-03-01T00:00:00Z', \"1d\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
